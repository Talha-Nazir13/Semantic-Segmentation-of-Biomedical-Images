{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c984608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d7e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarwickDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images_dir = os.path.join(root_dir, 'Train' if train else 'Test', 'images')\n",
    "        self.masks_dir = os.path.join(root_dir, 'Train' if train else 'Test', 'masks')\n",
    "\n",
    "        self.ids = [os.path.splitext(file)[0] for file in os.listdir(self.images_dir)\n",
    "                    if not file.startswith('.')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_id + '.png')\n",
    "        mask_path = os.path.join(self.masks_dir, img_id + '_anno.png')\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c726b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.down1 = ConvBlock(3, 64)\n",
    "        self.down2 = ConvBlock(64, 128)\n",
    "        self.down3 = ConvBlock(128, 256)\n",
    "        self.down4 = ConvBlock(256, 512)\n",
    "\n",
    "        self.up1 = ConvBlock(512, 256)\n",
    "        self.up2 = ConvBlock(256, 128)\n",
    "        self.up3 = ConvBlock(128, 64)\n",
    "        \n",
    "        self.trans1 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1)\n",
    "        self.trans2 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
    "        self.trans3 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
    "        \n",
    "        self.out = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        \n",
    "        x = self.trans1(x4)\n",
    "        x = self.up1(x + x3)\n",
    "        x = self.trans2(x)\n",
    "        x = self.up2(x + x2)\n",
    "        x = self.trans3(x)\n",
    "        x = self.up3(x + x1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ed1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.images = [os.path.join(root_dir, file) for file in os.listdir(root_dir) if file.endswith('.png')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        mask_path = img_path.replace('.png', '_mask.png')\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = SegmentationDataset('C:/Users/HP/Downloads/WARWICK/Train', transforms=transform)\n",
    "test_dataset = SegmentationDataset('C:/Users/HP/Downloads/WARWICK/Test', transforms=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e777f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = SegNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad727e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.2426, Train Acc: 0.9271, Test Loss: 0.0662, Test Acc: 0.9790\n",
      "Epoch 2/10, Train Loss: 0.0656, Train Acc: 0.9797, Test Loss: 0.0494, Test Acc: 0.9832\n",
      "Epoch 3/10, Train Loss: 0.0479, Train Acc: 0.9848, Test Loss: 0.0453, Test Acc: 0.9845\n",
      "Epoch 4/10, Train Loss: 0.0393, Train Acc: 0.9876, Test Loss: 0.0357, Test Acc: 0.9890\n",
      "Epoch 5/10, Train Loss: 0.0313, Train Acc: 0.9905, Test Loss: 0.0362, Test Acc: 0.9876\n",
      "Epoch 6/10, Train Loss: 0.0263, Train Acc: 0.9915, Test Loss: 0.0384, Test Acc: 0.9877\n",
      "Epoch 7/10, Train Loss: 0.0230, Train Acc: 0.9923, Test Loss: 0.0346, Test Acc: 0.9885\n",
      "Epoch 8/10, Train Loss: 0.0200, Train Acc: 0.9934, Test Loss: 0.0360, Test Acc: 0.9889\n",
      "Epoch 9/10, Train Loss: 0.0171, Train Acc: 0.9942, Test Loss: 0.0332, Test Acc: 0.9884\n",
      "Epoch 10/10, Train Loss: 0.0146, Train Acc: 0.9953, Test Loss: 0.0356, Test Acc: 0.9886\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# training\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "for epoch in range(10):  #  epoch count\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}')\n",
    "\n",
    "train_model(model, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb3deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
